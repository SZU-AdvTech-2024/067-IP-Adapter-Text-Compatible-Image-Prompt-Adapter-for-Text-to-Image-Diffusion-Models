{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "attn_maps = {}\n",
    "def hook_fn(name):\n",
    "    def forward_hook(module, input, output):\n",
    "        if hasattr(module.processor, \"attn_map\"):\n",
    "            attn_maps[name] = module.processor.attn_map\n",
    "            del module.processor.attn_map\n",
    "\n",
    "    return forward_hook\n",
    "\n",
    "def register_cross_attention_hook(unet):\n",
    "    for name, module in unet.named_modules():\n",
    "        if name.split('.')[-1].startswith('attn2'):\n",
    "            module.register_forward_hook(hook_fn(name))\n",
    "\n",
    "    return unet\n",
    "\n",
    "def upscale(attn_map, target_size):\n",
    "    attn_map = torch.mean(attn_map, dim=0)\n",
    "    attn_map = attn_map.permute(1,0)\n",
    "    temp_size = None\n",
    "\n",
    "    for i in range(0,5):\n",
    "        scale = 2 ** i\n",
    "        if ( target_size[0] // scale ) * ( target_size[1] // scale) == attn_map.shape[1]*64:\n",
    "            temp_size = (target_size[0]//(scale*8), target_size[1]//(scale*8))\n",
    "            break\n",
    "\n",
    "    assert temp_size is not None, \"temp_size cannot is None\"\n",
    "\n",
    "    attn_map = attn_map.view(attn_map.shape[0], *temp_size)\n",
    "\n",
    "    attn_map = F.interpolate(\n",
    "        attn_map.unsqueeze(0).to(dtype=torch.float32),\n",
    "        size=target_size,\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )[0]\n",
    "\n",
    "    attn_map = torch.softmax(attn_map, dim=0)\n",
    "    return attn_map\n",
    "\n",
    "def get_net_attn_map(image_size, batch_size=2, instance_or_negative=False, detach=True):\n",
    "\n",
    "    idx = 0 if instance_or_negative else 1\n",
    "    net_attn_maps = []\n",
    "\n",
    "    for name, attn_map in attn_maps.items():\n",
    "        attn_map = attn_map.cpu() if detach else attn_map\n",
    "        attn_map = torch.chunk(attn_map, batch_size)[idx].squeeze()\n",
    "        attn_map = upscale(attn_map, image_size) \n",
    "        net_attn_maps.append(attn_map) \n",
    "\n",
    "    net_attn_maps = torch.mean(torch.stack(net_attn_maps,dim=0),dim=0)\n",
    "\n",
    "    return net_attn_maps\n",
    "\n",
    "def attnmaps2images(net_attn_maps):\n",
    "\n",
    "    #total_attn_scores = 0\n",
    "    images = []\n",
    "\n",
    "    for attn_map in net_attn_maps:\n",
    "        attn_map = attn_map.cpu().numpy()\n",
    "        #total_attn_scores += attn_map.mean().item()\n",
    "\n",
    "        normalized_attn_map = (attn_map - np.min(attn_map)) / (np.max(attn_map) - np.min(attn_map)) * 255\n",
    "        normalized_attn_map = normalized_attn_map.astype(np.uint8)\n",
    "        #print(\"norm: \", normalized_attn_map.shape)\n",
    "        image = Image.fromarray(normalized_attn_map)\n",
    "\n",
    "        #image = fix_save_attn_map(attn_map)\n",
    "        images.append(image)\n",
    "\n",
    "    #print(total_attn_scores)\n",
    "    return images\n",
    "\n",
    "def is_torch2_available():\n",
    "    return hasattr(F, \"scaled_dot_product_attention\")\n",
    "\n",
    "def get_generator(seed, device):\n",
    "\n",
    "    if seed is not None:\n",
    "        if isinstance(seed, list):\n",
    "            generator = [torch.Generator(device).manual_seed(seed_item) for seed_item in seed]\n",
    "        else:\n",
    "            generator = torch.Generator(device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cal_l1_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "class AttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        # print(f\"hidden_states:{hidden_states.shape}\")\n",
    "        # print(f\"query:{query.shape}\")\n",
    "        # print(f\"key:{key.shape}\")\n",
    "        # print(f\"value:{value.shape}\")\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class KVAttnProcessor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "        mode=\"Texture\"\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.attnLoss = None\n",
    "        self.queryLoss = None\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        if self.mode == \"Texture\":\n",
    "            key_style = key[0:1]\n",
    "            value_style = value[0:1]\n",
    "            query_noise = query[1:2]\n",
    "            attn_style = F.scaled_dot_product_attention(query_noise, key_style, value_style, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "            attn_noise = hidden_states[1:2]\n",
    "            self.attnLoss = cal_l1_loss(attn_noise, attn_style)\n",
    "        elif self.mode == \"Style\":\n",
    "            key_style = key[0:1]\n",
    "            value_style = value[0:1]\n",
    "            query_noise = query[1:2]\n",
    "            query_content = query[2:3]\n",
    "            attn_style = F.scaled_dot_product_attention(query_noise, key_style, value_style, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "            attn_noise = hidden_states[1:2]\n",
    "            self.attnLoss = cal_l1_loss(attn_noise, attn_style)\n",
    "            self.queryLoss = cal_l1_loss(query_noise, query_content)\n",
    "\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "import inspect\n",
    "from torch import autocast\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms as tfms\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.utils import deprecate, logging, BaseOutput\n",
    "from diffusers.image_processor import PipelineImageInput\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "from diffusers.callbacks import PipelineCallback, MultiPipelineCallbacks\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "\n",
    "def GetLoss(unet,mode):\n",
    "    loss = torch.tensor(0.0, dtype=torch.float32).cuda()\n",
    "    for name, module in unet.attn_processors.items():\n",
    "        if name.endswith(\"attn1.processor\") & (\"up\" in name) & (\"up_blocks.1\" not in name):\n",
    "            loss += module.attnLoss\n",
    "            if mode == \"Style\":\n",
    "                loss += module.queryLoss * 0.2\n",
    "    return loss\n",
    "\n",
    "def GetLoss_EDIT(unet,mode):\n",
    "    loss = torch.tensor(0.0, dtype=torch.float32).cuda()\n",
    "    for name, module in unet.attn_processors.items():\n",
    "        if name.endswith(\"attn2.processor\"):\n",
    "            loss += module.attnLoss\n",
    "            if mode == \"Style\":\n",
    "                loss += module.queryLoss * 0.2\n",
    "    return loss\n",
    "\n",
    "class MyPipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float32)\n",
    "        self.vae = self.pipe.vae\n",
    "        self.tokenizer = self.pipe.tokenizer\n",
    "        self.text_encoder = self.pipe.text_encoder\n",
    "        self.unet = self.pipe.unet\n",
    "        self.scheduler = self.pipe.scheduler\n",
    "        # self.scheduler = DDIMScheduler(num_train_timesteps=1000,beta_start=0.00085,beta_end=0.012,beta_schedule=\"scaled_linear\",clip_sample=False,set_alpha_to_one=False,steps_offset=1,)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.vae = self.vae.to(self.device).requires_grad_(False)\n",
    "        self.text_encoder = self.text_encoder.to(self.device).requires_grad_(False)\n",
    "        self.unet = self.unet.to(self.device).requires_grad_(False)\n",
    "\n",
    "    def set_attention(self, mode):\n",
    "        attn_procs = {}\n",
    "        for name in self.unet.attn_processors.keys():\n",
    "            if name.endswith(\"attn1.processor\") & (\"up\" in name) & (\"up_blocks.1\" not in name):\n",
    "                attn_procs[name] = KVAttnProcessor(mode=mode)\n",
    "            else:\n",
    "                attn_procs[name] = AttnProcessor()\n",
    "        self.unet.set_attn_processor(attn_procs)\n",
    "    \n",
    "    def sample(self, style_path, content_path=None, num_inference_steps=200, size=512):\n",
    "        mode = \"Texture\"\n",
    "        if content_path is not None:\n",
    "            mode = \"Style\"\n",
    "        self.set_attention(mode)\n",
    "\n",
    "        # latents\n",
    "        latent_size = int(size/8)\n",
    "        noisy_latents = randn_tensor((1, 4, latent_size, latent_size), device=self.device).to(torch.float32)\n",
    "        # noisy_latents = noisy_latents * self.scheduler.init_noise_sigma\n",
    "        # style\n",
    "        style_image = Image.open(style_path).convert('RGB').resize((size,size))\n",
    "        style_latents = self.vae.encode(tfms.ToTensor()(style_image).unsqueeze(0).cuda() * 2 - 1)\n",
    "        style_latents = 0.18215 * style_latents.latent_dist.sample()\n",
    "        # content\n",
    "        if mode == \"Style\":\n",
    "            content_image = Image.open(content_path).convert('RGB').resize((size,size))\n",
    "            content_latents = self.vae.encode(tfms.ToTensor()(content_image).unsqueeze(0).cuda() * 2 - 1)\n",
    "            content_latents = 0.18215 * content_latents.latent_dist.sample()\n",
    "\n",
    "        # text\n",
    "        prompt = \"\"\n",
    "        text_input = self.tokenizer(prompt, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "        if mode == \"Texture\":\n",
    "            text_embeddings = torch.cat([text_embeddings, text_embeddings])\n",
    "        elif mode == \"Style\":\n",
    "            text_embeddings = torch.cat([text_embeddings, text_embeddings, text_embeddings])\n",
    "\n",
    "        # scheduler\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        # optimizer\n",
    "        # noisy_latents=noisy_latents.detach()\n",
    "        optimizer = torch.optim.Adam([noisy_latents.requires_grad_(True)], lr=0.05)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            for i,t in tqdm(enumerate(self.scheduler.timesteps), total=len(self.scheduler.timesteps)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if mode == \"Texture\":\n",
    "                    latent_model_input = torch.cat([style_latents.detach(), noisy_latents])\n",
    "                elif mode == \"Style\":\n",
    "                    latent_model_input = torch.cat([style_latents.detach(), noisy_latents, content_latents.detach()])\n",
    "                # latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)\n",
    "\n",
    "                loss = GetLoss(self.unet, mode)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # save\n",
    "                # temp_latent = 1 / 0.18215 * noisy_latents.clone()\n",
    "                # temp_image = self.vae.decode(temp_latent).sample\n",
    "                # temp_image = (temp_image / 2 + 0.5).clamp(0, 1)\n",
    "                # temp_image = temp_image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "                # temp_image = (temp_image * 255).round().astype(\"uint8\")\n",
    "                # pil_images = [Image.fromarray(temp_image) for temp_image in temp_image]\n",
    "                # pil_images[0].save(f\"results/{i}.png\")\n",
    "\n",
    "        noisy_latents = 1 / 0.18215 * noisy_latents\n",
    "        image = self.vae.decode(noisy_latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        \n",
    "        return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "pipe = MyPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 纹理生成\n",
    "style_image_path = \"纹理/208.jpg\"\n",
    "images = pipe.sample(style_path=style_image_path, num_inference_steps=150, size=512)\n",
    "images[0].save(\"outputs/ADLoss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下为用AD Loss进行风格迁移的代码\n",
    "# style_image_path = \"\"\n",
    "# content_image_path = \"\"\n",
    "# images = pipe.sample(style_path=style_image_path, content_path=content_image_path, num_inference_steps=500, size=512)\n",
    "# images[0].save(\"outputs/ADLoss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
