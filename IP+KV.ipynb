{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import inspect\n",
    "import PIL.Image\n",
    "from torchvision import transforms as tfms\n",
    "from diffusers.utils import deprecate, logging, BaseOutput\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.image_processor import PipelineImageInput\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "from diffusers.callbacks import PipelineCallback, MultiPipelineCallbacks\n",
    "from einops import rearrange\n",
    "from icecream import ic\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    sigmas: Optional[List[float]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if timesteps is not None and sigmas is not None:\n",
    "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    elif sigmas is not None:\n",
    "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accept_sigmas:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):\n",
    "    \"\"\"\n",
    "    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and\n",
    "    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4\n",
    "    \"\"\"\n",
    "    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)), keepdim=True)\n",
    "    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)\n",
    "    # rescale the results from guidance (fixes overexposure)\n",
    "    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n",
    "    # mix with the original results from guidance by factor guidance_rescale to avoid \"plain looking\" images\n",
    "    noise_cfg = guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n",
    "    return noise_cfg\n",
    "\n",
    "def adain_latent(feat, cond_feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size() # [1, 4, 1, 64, 64]\n",
    "    C = size[1]\n",
    "    feat_var = feat.view(C, -1).var(dim=1) + eps\n",
    "    feat_std = feat_var.sqrt().view(1, C, 1, 1)\n",
    "    feat_mean = feat.view(C, -1).mean(dim=1).view(1, C, 1, 1)\n",
    "    \n",
    "    cond_feat_var = cond_feat.view(C, -1).var(dim=1) + eps\n",
    "    cond_feat_std = cond_feat_var.sqrt().view(1, C, 1, 1)\n",
    "    cond_feat_mean = cond_feat.view(C, -1).mean(dim=1).view(1, C, 1, 1)\n",
    "    feat = (feat - feat_mean.expand(size)) / feat_std.expand(size)\n",
    "    return feat * cond_feat_std.expand(size) + cond_feat_mean.expand(size)\n",
    "\n",
    "class CustomStableDiffusionPipeline(StableDiffusionPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        refer_latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end: Optional[\n",
    "            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        ] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        # to deal with lora scaling and other possible forward hooks\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None)\n",
    "        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            self.do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, negative_prompt_embeds, prompt_embeds, prompt_embeds])\n",
    "        else:\n",
    "            prompt_embeds = torch.cat([prompt_embeds, prompt_embeds])\n",
    "\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps, sigmas)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "        if refer_latents is not None:\n",
    "            latents=torch.cat([latents,latents])\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "        # 6.1 Add image embeds for IP-Adapter\n",
    "        added_cond_kwargs = ({\"image_embeds\": image_embeds} if (ip_adapter_image is not None or ip_adapter_image_embeds is not None) else None)\n",
    "        # 6.2 Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "\n",
    "                if refer_latents is not None:\n",
    "                    _,latent_cur=latents.chunk(2)\n",
    "                    latent_ref=refer_latents[-i-1].unsqueeze(0)\n",
    "                    latents = torch.cat([latent_ref, latent_cur])\n",
    "\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    timestep_cond=timestep_cond,\n",
    "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[\n",
    "                0\n",
    "            ]\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "        else:\n",
    "            image = latents\n",
    "            has_nsfw_concept = None\n",
    "\n",
    "        if has_nsfw_concept is None:\n",
    "            do_denormalize = [True] * image.shape[0]\n",
    "        else:\n",
    "            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
    "\n",
    "        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def invert(\n",
    "        self,\n",
    "        image: Union[torch.Tensor, PIL.Image.Image] = None,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end: Optional[\n",
    "            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        ] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        # to deal with lora scaling and other possible forward hooks\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None)\n",
    "        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            self.do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps, sigmas)\n",
    "        timesteps = reversed(self.scheduler.timesteps)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        image = image.resize((512, 512))\n",
    "        latents = self.vae.encode(tfms.ToTensor()(image).unsqueeze(0).cuda().half() * 2 - 1)\n",
    "        latents = 0.18215 * latents.latent_dist.sample()\n",
    "        latents.requires_grad = False\n",
    "        intermediate_latents = []\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "        # 6.1 Add image embeds for IP-Adapter\n",
    "        added_cond_kwargs = ({\"image_embeds\": image_embeds} if (ip_adapter_image is not None or ip_adapter_image_embeds is not None) else None)\n",
    "        # 6.2 Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    timestep_cond=timestep_cond,\n",
    "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "                # inversion param\n",
    "                current_t = max(0, t.item() - (1000 // num_inference_steps))\n",
    "                next_t = t\n",
    "                alpha_t = self.scheduler.alphas_cumprod[current_t]\n",
    "                alpha_t_next = self.scheduler.alphas_cumprod[next_t]\n",
    "                # Inverted update step\n",
    "                latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (1 - alpha_t_next).sqrt() * noise_pred\n",
    "                # save invert output\n",
    "                # image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "                # image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "                # Store\n",
    "                intermediate_latents.append(latents)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "            # image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            # image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "            # image[0].save(f\"IP+KV_output/output_noisy_Image.png\")\n",
    "\n",
    "            return torch.cat(intermediate_latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "    \n",
    "class kvAttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use=0\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        self.use+=1\n",
    "        if self.use>33:\n",
    "            # KV替换\n",
    "            len=query.shape[0]\n",
    "            for x in range(1,len,2):\n",
    "                key[x]=key[x-1]\n",
    "                value[x]=value[x-1]\n",
    "\n",
    "        # # KV替换\n",
    "        # len=query.shape[0]\n",
    "        # for x in range(1,len,2):\n",
    "        #     key[x]=key[x-1]\n",
    "        #     value[x]=value[x-1]\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class IPAttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater for PyTorch 2.0.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n",
    "            The context length of the image features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4, skip=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.scale = scale\n",
    "        self.num_tokens = num_tokens\n",
    "        self.skip = skip\n",
    "\n",
    "        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        else:\n",
    "            # get encoder_hidden_states, ip_hidden_states\n",
    "            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n",
    "            encoder_hidden_states, ip_hidden_states = (encoder_hidden_states[:, :end_pos, :], encoder_hidden_states[:, end_pos:, :],)\n",
    "            # print(encoder_hidden_states.shape)\n",
    "            # print(ip_hidden_states.shape)\n",
    "            if attn.norm_cross:\n",
    "                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        if not self.skip:\n",
    "            # for ip-adapter\n",
    "            ip_key = self.to_k_ip(ip_hidden_states)\n",
    "            ip_value = self.to_v_ip(ip_hidden_states)\n",
    "\n",
    "            ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "            ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "            # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "            # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "            ip_hidden_states = F.scaled_dot_product_attention(query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n",
    "            with torch.no_grad():\n",
    "                self.attn_map = query @ ip_key.transpose(-2, -1).softmax(dim=-1)\n",
    "                #print(self.attn_map.shape)\n",
    "\n",
    "            ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "            ip_hidden_states = ip_hidden_states.to(query.dtype)\n",
    "\n",
    "            hidden_states = hidden_states + self.scale * ip_hidden_states\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.controlnet import MultiControlNetModel\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "def get_generator(seed, device):\n",
    "\n",
    "    if seed is not None:\n",
    "        if isinstance(seed, list):\n",
    "            generator = [torch.Generator(device).manual_seed(seed_item) for seed_item in seed]\n",
    "        else:\n",
    "            generator = torch.Generator(device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "\n",
    "    return generator\n",
    "\n",
    "class ImageProjModel(torch.nn.Module):\n",
    "    \"\"\"图像编码映射模型(线性层+LN)\"\"\"\n",
    "\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens, self.cross_attention_dim)\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens\n",
    "\n",
    "class MLPProjModel(torch.nn.Module):\n",
    "    \"\"\"SD model with image prompt\"\"\"\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(clip_embeddings_dim, clip_embeddings_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(clip_embeddings_dim, cross_attention_dim),\n",
    "            torch.nn.LayerNorm(cross_attention_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image_embeds):\n",
    "        clip_extra_context_tokens = self.proj(image_embeds)\n",
    "        return clip_extra_context_tokens\n",
    "\n",
    "class IPAdapter_Mine:\n",
    "    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4):\n",
    "        \"\"\"初始化\"\"\"\n",
    "        self.device = device\n",
    "        self.image_encoder_path = image_encoder_path\n",
    "        self.ip_ckpt = ip_ckpt\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.pipe = sd_pipe.to(self.device)\n",
    "        self.pipe.vae = self.pipe.vae.requires_grad_(False)\n",
    "        self.pipe.text_encoder = self.pipe.text_encoder.requires_grad_(False)\n",
    "        self.pipe.unet = self.pipe.unet.requires_grad_(False)\n",
    "\n",
    "        # load image encoder\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(self.device, dtype=torch.float16)\n",
    "        # image proj model\n",
    "        self.image_proj_model = self.init_proj()\n",
    "\n",
    "        self.set_ip_adapter()\n",
    "        self.load_ip_adapter()\n",
    "\n",
    "    def init_proj(self):\n",
    "        \"\"\"初始化映射模型\"\"\"\n",
    "        image_proj_model = ImageProjModel(\n",
    "            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n",
    "            clip_embeddings_dim=self.image_encoder.config.projection_dim,\n",
    "            clip_extra_context_tokens=self.num_tokens,\n",
    "        ).to(self.device, dtype=torch.float16)\n",
    "        return image_proj_model\n",
    "\n",
    "    def set_ip_adapter(self):\n",
    "        \"\"\"修改unet注意力块,把所有原始交叉注意力修改为解耦交叉注意力,上采样块的自注意力使用kv替换\"\"\"\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                # 自注意力\n",
    "                if (\"up\" in name) and (\"up_blocks.1\" not in name):\n",
    "                    attn_procs[name] = kvAttnProcessor()\n",
    "                else:\n",
    "                    attn_procs[name] = AttnProcessor()\n",
    "            else:\n",
    "                # 交叉注意力\n",
    "                attn_procs[name] = IPAttnProcessor(\n",
    "                    hidden_size=hidden_size,\n",
    "                    cross_attention_dim=cross_attention_dim,\n",
    "                    scale=1.0,\n",
    "                    num_tokens=self.num_tokens,\n",
    "                ).to(self.device, dtype=torch.float16)\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    def load_ip_adapter(self):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n",
    "            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n",
    "            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n",
    "                for key in f.keys():\n",
    "                    if key.startswith(\"image_proj.\"):\n",
    "                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n",
    "                    elif key.startswith(\"ip_adapter.\"):\n",
    "                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n",
    "        else:\n",
    "            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n",
    "        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n",
    "        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image=None, clip_image_embeds=None):\n",
    "        \"\"\"对图像进行CLIP编码，然后经过映射模型\"\"\"\n",
    "        if pil_image is not None:\n",
    "            if isinstance(pil_image, Image.Image):\n",
    "                pil_image = [pil_image]\n",
    "            clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "            clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtype=torch.float16)).image_embeds\n",
    "        else:\n",
    "            clip_image_embeds = clip_image_embeds.to(self.device, dtype=torch.float16)\n",
    "        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n",
    "        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds\n",
    "\n",
    "    def set_scale(self, scale):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            if isinstance(attn_processor, IPAttnProcessor):\n",
    "                attn_processor.scale = scale\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        pil_image=None,\n",
    "        clip_image_embeds=None,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        num_samples=4,\n",
    "        seed=None,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        ddim_latents=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        # 初始化text prompt\n",
    "        if pil_image is not None:\n",
    "            num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n",
    "        else:\n",
    "            num_prompts = clip_image_embeds.size(0)\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        # image prompt embds\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image=pil_image, clip_image_embeds=clip_image_embeds)\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        # 编码文本prompt，与image embeds做拼接\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=num_samples,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "            # print(prompt_embeds_.shape)\n",
    "            # print(image_prompt_embeds.shape)\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = get_generator(seed, self.device)\n",
    "\n",
    "        # 调用管线生成\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            refer_latents=ddim_latents,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images\n",
    "    \n",
    "    def invert(\n",
    "        self,\n",
    "        pil_image=None,\n",
    "        clip_image_embeds=None,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        num_samples=4,\n",
    "        seed=None,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        # 初始化text prompt\n",
    "        if pil_image is not None:\n",
    "            num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n",
    "        else:\n",
    "            num_prompts = clip_image_embeds.size(0)\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        # image prompt embds\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image=pil_image, clip_image_embeds=clip_image_embeds)\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        # my_ipe = torch.zeros_like(image_prompt_embeds)\n",
    "        # my_uipe = torch.zeros_like(uncond_image_prompt_embeds)\n",
    "\n",
    "        # 编码文本prompt，与image embeds做拼接\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=num_samples,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "            # or\n",
    "            # prompt_embeds = torch.cat([prompt_embeds_, my_ipe], dim=1)\n",
    "            # negative_prompt_embeds = torch.cat([negative_prompt_embeds_, my_uipe], dim=1)\n",
    "\n",
    "        generator = get_generator(seed, self.device)\n",
    "\n",
    "        # 调用管线生成\n",
    "        invert_latents = self.pipe.invert(\n",
    "            image=pil_image,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return invert_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"models/image_encoder/\"\n",
    "ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "pipe = CustomStableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image & prompt\n",
    "image_path = \"纹理/208.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "invert_latents = pipe.invert(image=image, prompt=\"\", guidance_scale=7.5, num_inference_steps=50)\n",
    "ip_model = IPAdapter_Mine(pipe, image_encoder_path, ip_ckpt, device)\n",
    "images = ip_model.generate(pil_image=image, num_samples=1, prompt=\"\",num_inference_steps=50, seed=50, scale=1, guidance_scale=7.5, ddim_latents=invert_latents)\n",
    "images[1].save(\"outputs/IP+KV.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
