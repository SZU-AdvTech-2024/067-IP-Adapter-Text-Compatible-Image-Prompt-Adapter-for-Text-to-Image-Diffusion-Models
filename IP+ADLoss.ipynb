{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import inspect\n",
    "from torchvision import transforms as tfms\n",
    "from diffusers.utils import deprecate, logging, BaseOutput\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.image_processor import PipelineImageInput\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "from diffusers.callbacks import PipelineCallback, MultiPipelineCallbacks\n",
    "from einops import rearrange\n",
    "from icecream import ic\n",
    "from PIL import Image\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "def GetLoss(unet):\n",
    "    loss = torch.tensor(0.0, dtype=torch.float32).cuda()\n",
    "    for name, module in unet.attn_processors.items():\n",
    "        if name.endswith(\"attn1.processor\") & (\"up\" in name) & (\"up_blocks.1\" not in name):\n",
    "            loss += module.attnLoss\n",
    "    return loss\n",
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    sigmas: Optional[List[float]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n",
    "            must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n",
    "            `num_inference_steps` and `sigmas` must be `None`.\n",
    "        sigmas (`List[float]`, *optional*):\n",
    "            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n",
    "            `num_inference_steps` and `timesteps` must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None and sigmas is not None:\n",
    "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    elif sigmas is not None:\n",
    "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accept_sigmas:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):\n",
    "    \"\"\"\n",
    "    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and\n",
    "    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4\n",
    "    \"\"\"\n",
    "    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)), keepdim=True)\n",
    "    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)\n",
    "    # rescale the results from guidance (fixes overexposure)\n",
    "    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n",
    "    # mix with the original results from guidance by factor guidance_rescale to avoid \"plain looking\" images\n",
    "    noise_cfg = guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n",
    "    return noise_cfg\n",
    "\n",
    "def adain_latent(feat, cond_feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size() # [1, 4, 1, 64, 64]\n",
    "    C = size[1]\n",
    "    feat_var = feat.view(C, -1).var(dim=1) + eps\n",
    "    feat_std = feat_var.sqrt().view(1, C, 1, 1)\n",
    "    feat_mean = feat.view(C, -1).mean(dim=1).view(1, C, 1, 1)\n",
    "    \n",
    "    cond_feat_var = cond_feat.view(C, -1).var(dim=1) + eps\n",
    "    cond_feat_std = cond_feat_var.sqrt().view(1, C, 1, 1)\n",
    "    cond_feat_mean = cond_feat.view(C, -1).mean(dim=1).view(1, C, 1, 1)\n",
    "    feat = (feat - feat_mean.expand(size)) / feat_std.expand(size)\n",
    "    return feat * cond_feat_std.expand(size) + cond_feat_mean.expand(size)\n",
    "\n",
    "class CustomStableDiffusionPipeline(StableDiffusionPipeline):\n",
    "    # @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        pil_image,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end: Optional[\n",
    "            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        ] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        kv = 0.5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        # to deal with lora scaling and other possible forward hooks\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None)\n",
    "        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            self.do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps, sigmas)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        ref_latents = self.vae.encode(tfms.ToTensor()(pil_image).unsqueeze(0).cuda() * 2 - 1)\n",
    "        ref_latents = 0.18215 * ref_latents.latent_dist.sample()\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "        # 6.1 Add image embeds for IP-Adapter\n",
    "        added_cond_kwargs = ({\"image_embeds\": image_embeds} if (ip_adapter_image is not None or ip_adapter_image_embeds is not None) else None)\n",
    "        # 6.2 Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        self._num_timesteps = len(timesteps)\n",
    "\n",
    "        kv_time = 1000*kv\n",
    "\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # normal sample\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = self.unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        timestep_cond=timestep_cond,\n",
    "                        cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                        added_cond_kwargs=added_cond_kwargs,\n",
    "                        return_dict=False,\n",
    "                    )[0]\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "                # loss optim\n",
    "                copy = ref_latents.clone()\n",
    "                latents = latents.detach()\n",
    "                optimizer = torch.optim.Adam([latents.requires_grad_(True)], lr=0.05)\n",
    "                latent_model_input = torch.cat([copy.detach(), latents])\n",
    "                _ = self.unet(latent_model_input,t,encoder_hidden_states=prompt_embeds,timestep_cond=timestep_cond,cross_attention_kwargs=self.cross_attention_kwargs,added_cond_kwargs=added_cond_kwargs,return_dict=False,)[0]\n",
    "                loss = GetLoss(self.unet)\n",
    "                if(t < kv_time):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "        latents=latents.detach()\n",
    "        if not output_type == \"latent\":\n",
    "            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[0]\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "        else:\n",
    "            image = latents\n",
    "            has_nsfw_concept = None\n",
    "\n",
    "        if has_nsfw_concept is None:\n",
    "            do_denormalize = [True] * image.shape[0]\n",
    "        else:\n",
    "            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
    "        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cal_l1_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "class AttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class kvAttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "    ):\n",
    "        self.attnLoss = None\n",
    "        self.forLoss = False\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        if (self.forLoss):\n",
    "            key_style = key[0:1]\n",
    "            value_style = value[0:1]\n",
    "            query_noise = query[1:2]\n",
    "            attn_style = F.scaled_dot_product_attention(query_noise, key_style, value_style, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "            attn_noise = hidden_states[1:2]\n",
    "            self.attnLoss = cal_l1_loss(attn_noise, attn_style)\n",
    "        self.forLoss = not(self.forLoss)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class IPAttnProcessor(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater for PyTorch 2.0.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n",
    "            The context length of the image features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4, skip=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.scale = scale\n",
    "        self.num_tokens = num_tokens\n",
    "        self.skip = skip\n",
    "        self.time = 0\n",
    "        self.use = 0\n",
    "\n",
    "        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        else:\n",
    "            # get encoder_hidden_states, ip_hidden_states\n",
    "            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n",
    "            encoder_hidden_states, ip_hidden_states = (\n",
    "                encoder_hidden_states[:, :end_pos, :],\n",
    "                encoder_hidden_states[:, end_pos:, :],\n",
    "            )\n",
    "            if attn.norm_cross:\n",
    "                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # self.use+=1\n",
    "        # if self.use > 0:\n",
    "        #     self.skip = True\n",
    "        if not self.skip:\n",
    "            # for ip-adapter\n",
    "            ip_key = self.to_k_ip(ip_hidden_states)\n",
    "            ip_value = self.to_v_ip(ip_hidden_states)\n",
    "\n",
    "            ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "            ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "            # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "            # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "            ip_hidden_states = F.scaled_dot_product_attention(\n",
    "                query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                self.attn_map = query @ ip_key.transpose(-2, -1).softmax(dim=-1)\n",
    "                #print(self.attn_map.shape)\n",
    "\n",
    "            ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "            ip_hidden_states = ip_hidden_states.to(query.dtype)\n",
    "\n",
    "            hidden_states = hidden_states + self.scale * ip_hidden_states\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.controlnet import MultiControlNetModel\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "def get_generator(seed, device):\n",
    "\n",
    "    if seed is not None:\n",
    "        if isinstance(seed, list):\n",
    "            generator = [torch.Generator(device).manual_seed(seed_item) for seed_item in seed]\n",
    "        else:\n",
    "            generator = torch.Generator(device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "\n",
    "    return generator\n",
    "\n",
    "class ImageProjModel(torch.nn.Module):\n",
    "    \"\"\"Projection Model\"\"\"\n",
    "\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens, self.cross_attention_dim)\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens\n",
    "\n",
    "class MLPProjModel(torch.nn.Module):\n",
    "    \"\"\"SD model with image prompt\"\"\"\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(clip_embeddings_dim, clip_embeddings_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(clip_embeddings_dim, cross_attention_dim),\n",
    "            torch.nn.LayerNorm(cross_attention_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image_embeds):\n",
    "        clip_extra_context_tokens = self.proj(image_embeds)\n",
    "        return clip_extra_context_tokens\n",
    "\n",
    "class IPAdapter:\n",
    "    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4):\n",
    "        self.device = device\n",
    "        self.image_encoder_path = image_encoder_path\n",
    "        self.ip_ckpt = ip_ckpt\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.pipe = sd_pipe.to(self.device)\n",
    "        self.pipe.vae = self.pipe.vae.requires_grad_(False)\n",
    "        self.pipe.text_encoder = self.pipe.text_encoder.requires_grad_(False)\n",
    "        self.pipe.unet = self.pipe.unet.requires_grad_(False)\n",
    "        self.set_ip_adapter()\n",
    "\n",
    "        # load image encoder\n",
    "        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(self.device, dtype=torch.float32)\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "        # image proj model\n",
    "        self.image_proj_model = self.init_proj()\n",
    "\n",
    "        self.load_ip_adapter()\n",
    "\n",
    "    def init_proj(self):\n",
    "        image_proj_model = ImageProjModel(\n",
    "            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n",
    "            clip_embeddings_dim=self.image_encoder.config.projection_dim,\n",
    "            clip_extra_context_tokens=self.num_tokens,\n",
    "        ).to(self.device, dtype=torch.float32)\n",
    "        return image_proj_model\n",
    "\n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                # 自注意力\n",
    "                if (\"up\" in name) and (\"up_blocks.1\" not in name):\n",
    "                    attn_procs[name] = kvAttnProcessor()\n",
    "                else:\n",
    "                    attn_procs[name] = AttnProcessor()\n",
    "            else:\n",
    "                # 交叉注意力\n",
    "                attn_procs[name] = IPAttnProcessor(\n",
    "                    hidden_size=hidden_size,\n",
    "                    cross_attention_dim=cross_attention_dim,\n",
    "                    scale=1.0,\n",
    "                    num_tokens=self.num_tokens,\n",
    "                ).to(self.device, dtype=torch.float32)\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    def load_ip_adapter(self):\n",
    "        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n",
    "            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n",
    "            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n",
    "                for key in f.keys():\n",
    "                    if key.startswith(\"image_proj.\"):\n",
    "                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n",
    "                    elif key.startswith(\"ip_adapter.\"):\n",
    "                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n",
    "        else:\n",
    "            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n",
    "        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n",
    "        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image=None, clip_image_embeds=None):\n",
    "        if pil_image is not None:\n",
    "            if isinstance(pil_image, Image.Image):\n",
    "                pil_image = [pil_image]\n",
    "            clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "            clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtype=torch.float32)).image_embeds\n",
    "        else:\n",
    "            clip_image_embeds = clip_image_embeds.to(self.device, dtype=torch.float32)\n",
    "        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n",
    "        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds\n",
    "\n",
    "    def set_scale(self, scale):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            if isinstance(attn_processor, IPAttnProcessor):\n",
    "                attn_processor.scale = scale\n",
    "\n",
    "    def set_time(self, time):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            if isinstance(attn_processor, IPAttnProcessor):\n",
    "                attn_processor.time = time\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        pil_image_path=None,\n",
    "        clip_image_embeds=None,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        num_samples=4,\n",
    "        seed=None,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        kv=0.5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        image_for_pipe = Image.open(pil_image_path).resize((512, 512))\n",
    "\n",
    "        image_for_prompt = Image.open(pil_image_path)\n",
    "\n",
    "        if image_for_prompt is not None:\n",
    "            num_prompts = 1 if isinstance(image_for_prompt, Image.Image) else len(image_for_prompt)\n",
    "        else:\n",
    "            num_prompts = clip_image_embeds.size(0)\n",
    "\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image=image_for_prompt, clip_image_embeds=clip_image_embeds)\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=num_samples,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = get_generator(seed, self.device)\n",
    "\n",
    "        images = self.pipe(\n",
    "            image_for_pipe,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            kv = kv,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"models/image_encoder/\"\n",
    "ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float32)\n",
    "\n",
    "pipe = CustomStableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ").to(device)\n",
    "\n",
    "ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"纹理/208.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ip_model.generate(pil_image_path=image_path, num_samples=1, scale=1, guidance_scale=7.5, num_inference_steps=50, seed=50, kv=0.3)\n",
    "grid = image_grid(images, 1, 1)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].save(\"outputs/IP+ADLoss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
